# -*- coding: utf-8 -*-
"""003_SOCIAL_MEDIA_PARAMETROS_Nivel_Pyspark

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nitGJWjnnipgUPr-m8hntc1VHwyXvaN1

#**Importações**

**Instalações de bibliotecas não nativas**
"""

pip install pyspark

pip install fsspec

pip install gcsfs

"""**BIBLIOTECAS**"""

import os
from google.cloud import storage

from pyspark.sql import SparkSession, Window
spark = SparkSession.builder.appName('GCSFilesRead').getOrCreate()

import pyspark.sql.functions as F
from pyspark.sql.functions import to_timestamp, to_date

from pyspark.sql.types import *
import pandas as pd

"""**Criando esquema e importando dataframe**"""

customSchema = StructType([
   StructField("id_video", StringType(),True),
   StructField("titulo_video", StringType(),True),
   StructField("publicado_em", StringType(),True),
   StructField("id_canal", StringType(),True),
   StructField("nome_canal", StringType(),True),
   StructField("data_destaque", StringType(),True),
   StructField("cont_visualizacao", IntegerType(),True),
   StructField("curtidas", IntegerType(),True),
   StructField("nao_curtidas", IntegerType(),True),
   StructField("cont_comentarios", IntegerType(),True),
   StructField("comentarios_desabilitados", StringType(),True),
   StructField("curtidas_desabilitadas", StringType(),True),
   StructField("pais", StringType(),True),
   StructField("categoria", StringType(),True)

])

schema = customSchema

dfspark = pd.read_csv('gs://projetofinalgrupo8/saida/youtube_tratado_pandas.csv', sep=',',encoding='UTF-8', header=0)
df= spark.createDataFrame(dfspark,schema=schema)

df.show(5)

df.printSchema()

"""#**Criação e remoção de colunas**

**Criação colunas publicação de data e data de destaque**
"""

#CRIAR COLUNAS DE PUBLICAÇÃO DE DATA E DATA DE DESTAQUE
df = df.withColumn("publicado_em_data", F.lit(F.substring('publicado_em', 1, 10))).withColumn("data_destaque1", F.lit(F.substring('data_destaque', 1, 10)))
df.show(1)

"""**Remoção colunas publicado_em e data_destaque**"""

#DROPAR COLUNA PUBLICADO_EM
df = df.drop('publicado_em')
df.show(1)

#DROPAR COLUNA DE DESTAQUE
df = df.drop('data_destaque')
df.show(1)

"""**Renomeando coluna data destaque**"""

df = df.withColumnRenamed("data_destaque1", "data_destaque")
df.show(1)

"""#**Criação de Insights**

**Relação curtida por visualização em porcentagem**
"""

df = (df.withColumn("like_por_visualizacao_100", F.round((F.col("curtidas")/ F.col("cont_visualizacao")*100),2))) 
df.show(1)

"""**Relação descurtida por visualização em porcentagem**

"""

df = (df.withColumn("deslike_por_visualizacao_100", F.round((F.col("nao_curtidas")/ F.col("cont_visualizacao")*100),2))) 

df.show(1)

"""**Relação de comentários por visualização em porcentagem**

"""

df = (df.withColumn("coment_por_visualizacao_100", F.round((F.col("cont_comentarios")/ F.col("cont_visualizacao")*100),2))) 
df.show(1)

"""#**Transformando os tipos das colunas para data**


"""

df = df.withColumn("publicado_em_data", F.col("publicado_em_data").cast("date"))
df = df.withColumn("data_destaque", F.col("data_destaque").cast("date"))

df.select(F.col("publicado_em_data"), F.col("data_destaque")).show(5)
df.printSchema()

"""#**Rank de videos dentro de um canal**

"""

#RANK DOS VÍDEOS COM MAIS CURTIDA/VISUALIZAÇÃO EM UM CANAL
w0 = Window.partitionBy(F.col("nome_canal")).orderBy("like_por_visualizacao_100")

(df.withColumn("RANK",F.rank().over(w0))
  .withColumn("curtida_por_visualizacao",F.max(F.col("like_por_visualizacao_100")).over(w0))
  .select("RANK","curtida_por_visualizacao",'titulo_video','nome_canal',"pais","publicado_em_data").show(15)
)

#RANK DOS VÍDEOS COM MAIS DESCURTIDA/VISUALIZAÇÃO EM UM CANAL
w0 = Window.partitionBy(F.col("nome_canal")).orderBy("deslike_por_visualizacao_100")

(df.withColumn("RANK",F.row_number().over(w0))
  .withColumn("descurtida_por_visualizacao",F.max(F.col("deslike_por_visualizacao_100")).over(w0))
  .select("RANK","descurtida_por_visualizacao",'titulo_video','nome_canal',"pais","publicado_em_data").show(15)
)

"""#**Destaques top 10**

**Videos com mais visualizações**
"""

(df.groupBy(F.col("titulo_video")).agg(
    F.max("cont_visualizacao").alias("total_visualizacao"),
    F.max("publicado_em_data").alias("publicado_em_data"),
    F.max("data_destaque").alias("data_destaque")).orderBy('total_visualizacao', ascending=False).show(10)
)

"""**Canais com maior soma de visualização**"""

(df.groupBy(F.col("nome_canal")).agg(
    F.sum("cont_visualizacao").alias("soma_visualizacao"),
    F.round(F.avg("cont_visualizacao"),2).alias("media_visualizacao"),
    F.sum("curtidas").alias("soma_curtidas"),
    F.round(F.avg("curtidas"),2).alias("media_curtidas"),
    F.sum("nao_curtidas").alias("soma_naocurtidas"),
    F.max("publicado_em_data").alias("publicado_em_data"),
    F.max("data_destaque").alias("data_destaque"),
    F.round(F.avg("nao_curtidas"),2).alias("media_naocurtidas")).orderBy('soma_visualizacao', ascending=False).show(10)
)

"""**Videos com mais de 50% de curtidas em relação as visualizações**"""

df.where((F.col("pais") == 'BR')).filter("like_por_visualizacao_100 > 50").show(20)

"""#SALVANDO NO BUCKET

**Salvando na raiz do notebook, pois não foi possivel salvar o dataframe do pyspark na GCP**
"""

df.write.format("com.databricks.spark.csv").option("header", "true")\
                                           .option("inferschema", "true")\
                                           .option("delimiter", ",")\
                                           .save("/content/data_tratado_pyspark")

"""**Devido ao tamanho do arquivo ele se reparte em dois no salvamento, assim é necessário carrega-lo novamente para poder salvar através do pandas na GCP**"""

#COPIAR OS ENDEREÇOS DOS ARQUIVOS NA PASTA "data_tratado_pyspark"
df_pandas1 = pd.read_csv('/content/data_tratado_pyspark/part-00000-ca6ec06c-1839-48be-9def-887040fde7fb-c000.csv')
df_pandas2 = pd.read_csv('/content/data_tratado_pyspark/part-00001-ca6ec06c-1839-48be-9def-887040fde7fb-c000.csv')
df_pandas2.head(1)

"""**Verificação do tamanho dos datasets que foram repartidos**"""

print(df_pandas1.shape)
print(df_pandas2.shape)

"""**Concatenação e verificação do tamanho**"""

df_pandas_def = pd.concat([df_pandas1, df_pandas2])
df_pandas_def.shape

"""**Salvando no Bucket Final**"""

serviceAccount = '/content/projetofinalgrupo8-2dcd866c3f46.json'
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = serviceAccount

client = storage.Client()
bucket = client.get_bucket('projetofinalgrupo8')
bucket.blob('saida/youtube_tratado_pyspark.csv').upload_from_string(df_pandas_def.to_csv(index=False), 'text/csv')